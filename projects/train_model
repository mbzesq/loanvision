# backend/train_model.py

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC  # <-- Importing our new, more powerful algorithm
from sklearn.pipeline import make_pipeline
from sklearn.metrics import accuracy_score, classification_report
import joblib

def train_and_evaluate_model():
    """
    Loads the labeled dataset, splits it, trains a more advanced classifier,
    evaluates its performance, and saves the best model.
    """
    print("--- Starting V3 Model Training and Evaluation ---")
    
    input_filename = 'dataset_prelabeled.csv'
    try:
        df = pd.read_csv(input_filename)
    except FileNotFoundError:
        print(f"Error: '{input_filename}' not found. Please ensure you have created and labeled the dataset first.")
        return

    df.dropna(subset=['text', 'ground_truth_label'], inplace=True)
    df['ground_truth_label'] = df['ground_truth_label'].astype(str).str.strip()

    print(f"Loaded {len(df)} labeled pages.")
    
    label_counts = df['ground_truth_label'].value_counts()
    labels_to_keep = label_counts[label_counts >= 2].index
    df = df[df['ground_truth_label'].isin(labels_to_keep)]
    print(f"Using {len(df)} pages after removing rare classes for training.")

    # --- Step 2: Split Data ---
    X_train, X_test, y_train, y_test = train_test_split(
        df['text'], 
        df['ground_truth_label'], 
        test_size=0.2, 
        random_state=42,
        stratify=df['ground_truth_label']
    )
    print(f"\nData split into {len(X_train)} training samples and {len(X_test)} testing samples.")

    # --- Step 3: Define and Train the UPGRADED Model Pipeline ---
    # We've made two key upgrades here:
    # 1. TfidfVectorizer now uses n-grams to learn from 1, 2, and 3-word phrases.
    # 2. We are now using the more powerful LinearSVC algorithm.
    model_pipeline = make_pipeline(
        TfidfVectorizer(stop_words='english', ngram_range=(1, 3)),
        LinearSVC(random_state=42)
    )

    print("\nTraining the upgraded model on the training data...")
    model_pipeline.fit(X_train, y_train)
    print("Training complete.")

    # --- Step 4: Evaluate the Model ---
    print("\nEvaluating model performance on the unseen test data...")
    y_pred = model_pipeline.predict(X_test)
    
    accuracy = accuracy_score(y_test, y_pred)
    print(f"\nModel Accuracy: {accuracy:.2%}")
    
    print("\nClassification Report:")
    report_labels = sorted(list(y_train.unique()))
    print(classification_report(y_test, y_pred, labels=report_labels, zero_division=0))

    # --- Step 5: Save the Final Model ---
    if accuracy > 0.80:
        model_filename = 'doc_classifier_model.joblib'
        joblib.dump(model_pipeline, model_filename)
        print(f"\nModel performance is good. V3 model saved to '{model_filename}'")
    else:
        print("\nModel performance is below the 85% threshold. Model not saved. Consider improving features or adding more data.")

if __name__ == '__main__':
    train_and_evaluate_model()
